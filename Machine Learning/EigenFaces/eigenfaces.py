# -*- coding: utf-8 -*-
"""Copy of eigenfaces.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15r_sGsGj8LE-iHCBRe8q5g5HEJZxJxlW

## [Faces recognition example using eigenfaces and SVMs](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html)
"""

import tarfile
from time import time
import logging
import pylab as pl
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA as RandomizedPCA
from sklearn.svm import SVC

!wget http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tfile = tarfile.open("lfw-funneled.tgz", "r:gz")
# tfile.extractall(".")

# Display progress logs on stdout
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')

# Download the data, if not already on disk and load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape
np.random.seed(42)

n_samples, h, w

# for machine learning we use the data directly (as relative pixel
# position info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]

# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)

"""#### #1 Split into a training and testing set"""

# Split into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""#### #2 Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled dataset)

* unsupervised feature extraction / dimensionality reduction
"""

# the original dimensionality is over 1800
n_components = 150

print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))
t0 = time()
# create the PCA
pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)
print("done in %0.3fs" % (time() - t0))

# principle components of the face data
# reshape the data so they can look like pictures
eigenfaces = pca.components_.reshape((n_components, h, w))

print("Projecting the input data on the eigenfaces orthonormal basis")
t0 = time()
# transform the data into the principle component representation
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("done in %0.3fs" % (time() - t0))

"""#### #3 Train a SVM classification model"""

print("Fitting the classifier to the training set")
t0 = time()
param_grids = {
    'C': [1e3, 5e3, 1e4, 5e4, 1e5],
    'gamma':  [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]
}
# create the classifier
clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grids)
# fit the data
clf = clf.fit(X_train_pca, y_train)

print('done in %0.3fs' % (time() - t0))
print('Best estimator fount in grid search:')
print(clf.best_estimator_)

"""#### #4 Quantitative evaluation of the model quality on the test set"""

print("Predicting the people names on the testing set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print('done in %0.3fs' % (time() - t0))

print(classification_report(y_test, y_pred, target_names=target_names))
print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))

"""#### #5 Qualitative evaluation of the predictions using matplotlib"""

def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
    """Helper function to plot a gallery of portraits"""
    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
    for i in range(n_row * n_col):
        pl.subplot(n_row, n_col, i + 1)
        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)
        pl.title(titles[i], size=12)
        pl.xticks(())
        pl.yticks(())

"""#### #6 Plot the result of the prediction on a portion of the test set"""

def title(y_pred, y_test, target_names, i):
    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)

prediction_titles = [title(y_pred, y_test, target_names, i)
                         for i in range(y_pred.shape[0])]

plot_gallery(X_test, prediction_titles, h, w)

"""#### #7 Plot the gallery of the most significative eigenfaces"""

eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)

pl.show()

"""#### #8 How much of the variance is explained by the first principal component? The second?"""

pca.explained_variance_ratio_[0]

pca.explained_variance_ratio_[1]

"""#### #9 How many PCs to use? As you add more principal components as features for training your classifier, do you expect it to get better or worse performance?

Now you'll experiment with keeping different numbers of principal components. In a multiclass classification problem like this one (more than 2 labels to apply), accuracy is a less-intuitive metric than in the 2-class case. Instead, a popular metric is the F1 score.

While ideally, adding components should provide us additional signal to improve our performance, it is possible that we end up at a complexity where we overfit.
"""

# the original dimensionality is over 1800
#n_components = 150

for n_components in [10, 15, 25, 50, 100, 250]:
    print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))
    t0 = time()
    # create the PCA
    pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)
    print("done in %0.3fs" % (time() - t0))

    # principle components of the face data
    # reshape the data so they can look like pictures
    eigenfaces = pca.components_.reshape((n_components, h, w))

    print("Projecting the input data on the eigenfaces orthonormal basis")
    t0 = time()
    # transform the data into the principle component representation
    X_train_pca = pca.transform(X_train)
    X_test_pca = pca.transform(X_test)
    print("done in %0.3fs" % (time() - t0))

    print("Fitting the classifier to the training set")
    t0 = time()
    param_grids = {
        'C': [1e3, 5e3, 1e4, 5e4, 1e5],
        'gamma':  [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]
    }
    # create the classifier
    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grids)
    # fit the data
    clf = clf.fit(X_train_pca, y_train)

    print('done in %0.3fs' % (time() - t0))
    print('Best estimator fount in grid search:')
    print(clf.best_estimator_)

    print("Predicting the people names on the testing set")
    t0 = time()
    y_pred = clf.predict(X_test_pca)
    print('done in %0.3fs' % (time() - t0))

    print(classification_report(y_test, y_pred, target_names=target_names))
    print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))

"""#### #10 If you see a higher F1 score, does it mean the classifier is doing better, or worse?

Yes, higher means better!

#### #11 Do you see any evidence of overfitting when using a large number of PCs? Does the dimensionality reduction of PCA seem to be helping your performance here?

Yes, the F1 score starts to drop.

#### #12 Selecting a Number of Principal Components 

* train on different number of PCs and see how accuracy responds - cut off when it comes apparent that adding more PCs doesn't buy you more discrimination.

* this can be done also in feature selection as well. You don't want to do feature selection before going into PCA. PCA is going to combine information from potentially many different input features together
"""

